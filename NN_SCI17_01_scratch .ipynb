{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ccde21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install sklearn\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install hdfdict\n",
    "# !pip install prettytable\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ff4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import math\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import cm\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# !pip install hdfdict\n",
    "import hdfdict\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm,trange\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b804c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = mnist.load_data()\n",
    "\n",
    "# Preparing the data\n",
    "Y_tr_resh = Y_train_orig.reshape(60000, 1)\n",
    "Y_te_resh = Y_test_orig.reshape(10000, 1)\n",
    "Y_tr_T = to_categorical(Y_tr_resh, num_classes=10)\n",
    "Y_te_T = to_categorical(Y_te_resh, num_classes=10)\n",
    "y_tr = Y_tr_T.T\n",
    "y_te = Y_te_T.T\n",
    "\n",
    "\n",
    "# Flattening of inputs\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# Preprocessing of Inputs\n",
    "X_tr = X_train_flatten.T / 255.\n",
    "X_te = X_test_flatten.T / 255.\n",
    "num_classes = y_tr.shape[0]\n",
    "m_tr = X_tr.shape[0]\n",
    "m_te = X_te.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6202fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "  def forward(self):\n",
    "    return np.maximum(0,self)\n",
    "  \n",
    "  def backward(self):\n",
    "    self[self<=0] = 0\n",
    "    self[self>0] = 1\n",
    "    return self\n",
    "\n",
    "class Tanh:\n",
    "  def forward(self):\n",
    "    return np.tanh(self)\n",
    "\n",
    "  def backward(self):\n",
    "    return (1 - np.square(Tanh.forward(self)))\n",
    "\n",
    "class Sigmoid:\n",
    "  def forward(self):\n",
    "    return (1 / (1 + np.exp(-self)))\n",
    "  \n",
    "  def backward(self):\n",
    "    return (Sigmoid.forward(self)*(1-Sigmoid.forward(self)))\n",
    "\n",
    "class Softmax:\n",
    "  def forward(self):\n",
    "    soft = np.exp(self)/np.sum(np.exp(self),axis=0)\n",
    "    return soft\n",
    "\n",
    "  def backward(self):\n",
    "    return (Softmax.forward(self)*(1 - Softmax.forward(self))) \n",
    "\n",
    "class Sine:\n",
    "  def forward(self):\n",
    "    return np.sin(self)\n",
    "  \n",
    "  def backward(self):\n",
    "    return np.cos(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd26c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "  def __init__(self,A,threshold=0.5):\n",
    "    self.A = A\n",
    "    self.threshold = threshold\n",
    "  def __call__(self):\n",
    "    predictions = np.zeros((self.A.shape))\n",
    "    for g in range(0,self.A.shape[1]):\n",
    "      if self.A[:,g] >= self.threshold:\n",
    "        predictions[:,g] = 1\n",
    "    return predictions\n",
    "\n",
    "class PredictMulti:\n",
    "  def __init__(self,A):\n",
    "    self.A = A\n",
    "  def __call__(self):\n",
    "    predictions_multi = np.zeros(self.A.shape)\n",
    "    for v in range(0,self.A.shape[1]):\n",
    "      temp = max(self.A[:,v])\n",
    "      for w in range(0,self.A.shape[0]):\n",
    "        if self.A[w,v] == temp:\n",
    "          predictions_multi[w,v] = 1\n",
    "        else:\n",
    "          predictions_multi[w,v] = 0\n",
    "    return predictions_multi\n",
    "\n",
    "class Evaluate:\n",
    "  def __init__(self,y,preds):\n",
    "    self.y,self.preds = y,preds\n",
    "  def __call__(self):\n",
    "    accuracy = float(np.mean(self.preds==self.y,axis=1)*100)\n",
    "    return accuracy\n",
    "\n",
    "class EvaluateMulti:\n",
    "  def __init__(self,y,preds):\n",
    "    self.y,self.preds = y,preds\n",
    "  def __call__(self):\n",
    "    ones_array = np.ones(self.preds.shape)\n",
    "    temp1 = self.preds==ones_array\n",
    "    ind = []\n",
    "    for gee in range(0,temp1.shape[1]):\n",
    "      for jee in range(0,temp1.shape[0]):\n",
    "        if temp1[jee,gee] == True:\n",
    "          ind.append(jee)\n",
    "    ind_arr = np.array(ind)\n",
    "    y_list = []\n",
    "    for gee in range(0,self.y.shape[1]):\n",
    "      for jee in range(0,self.y.shape[0]):\n",
    "        if self.y[jee,gee] == 1:\n",
    "          y_list.append(jee)\n",
    "    y_arr = np.array(y_list)\n",
    "    accuracy = float(np.mean(ind_arr == y_arr.T))*100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class PrecisionRecall:\n",
    "  def __init__(self,A,y):\n",
    "    self.A,self.y = A,y\n",
    "  def __call__(self):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(0,self.y.shape[1]):\n",
    "      if ((self.A[0,i]==1)and(self.y[0,i]==1)):\n",
    "        tp = tp+1\n",
    "      if ((self.A[0,i]==1)and(self.y[0,i]==0)):\n",
    "        fp = fp+1\n",
    "      if (self.A[0,i]==0)and(self.y[0,i]==1):\n",
    "        fn = fn+1\n",
    "    prec = tp/(tp+fp)\n",
    "    rec = tp/(tp+fn)\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return prec,rec,f1\n",
    "\n",
    "class PrecisionRecallMulti:\n",
    "  def __init__(self,A,y):\n",
    "    self.A,self.y = A,y\n",
    "  def __call__(self):\n",
    "    epsilon = 1e-5\n",
    "    tp_multi = {}\n",
    "    fp_multi = {}\n",
    "    fn_multi = {}\n",
    "    prec_multi = {}\n",
    "    rec_multi = {}\n",
    "    f1_multi = {}\n",
    "    num_classes = self.y.shape[0]\n",
    "    for r in range(0,num_classes):\n",
    "      tp_multi[\"class\" + str(r)] = 0\n",
    "      fp_multi[\"class\" + str(r)] = 0\n",
    "      fn_multi[\"class\" + str(r)] = 0\n",
    "    for c in range(0,self.y.shape[1]):\n",
    "      for g in range(0,self.y.shape[0]):\n",
    "        if ((self.A[g,c]==1) and (self.y[g,c]==1)):\n",
    "          tp_multi[\"class\" + str(g)] = tp_multi[\"class\" + str(g)] + 1\n",
    "        if ((self.A[g,c]==1) and (self.y[g,c]==0)):\n",
    "          fp_multi[\"class\" + str(g)] = fp_multi[\"class\" + str(g)] + 1\n",
    "        if ((self.A[g,c]==0) and (self.y[g,c]==1)):\n",
    "          fn_multi[\"class\" + str(g)] = fn_multi[\"class\" + str(g)] + 1\n",
    "    for n in range(0,num_classes):\n",
    "      prec_multi[\"class\" + str(n)] = tp_multi[\"class\" + str(n)] / (tp_multi[\"class\" + str(n)] + fp_multi[\"class\" + str(n)] + epsilon)\n",
    "      rec_multi[\"class\" + str(n)] = tp_multi[\"class\" + str(n)] / (tp_multi[\"class\" + str(n)] + fn_multi[\"class\" + str(n)] + epsilon)\n",
    "      f1_multi[\"class\" + str(n)] = (2*prec_multi[\"class\" + str(n)]*rec_multi[\"class\" + str(n)])/(prec_multi[\"class\" + str(n)] + rec_multi[\"class\" + str(n)] + epsilon)\n",
    "    return prec_multi,rec_multi,f1_multi\n",
    "\n",
    "class GradL1Reg:\n",
    "  def __init__(self,layers_arr):\n",
    "    self.layers_arr = layers_arr\n",
    "  def __call__(self):\n",
    "    for layer in self.layers_arr:\n",
    "      layer.grad_L1 = np.zeros(layer.weights.shape)\n",
    "      for p in range(0,layer.weights.shape[0]):\n",
    "        for n in range(0,layer.weights.shape[1]):\n",
    "          if layer.weights[p,n] > 0:\n",
    "            layer.grad_L1[p,n] = 1\n",
    "          else:\n",
    "            layer.grad_L1[p,n] = -1\n",
    "\n",
    "class CreateMiniBatches:\n",
    "  def __init__(self,X,y,mb_size):\n",
    "    self.X,self.y,self.mb_size = X,y,mb_size\n",
    "  def __call__(self):\n",
    "    m_ex = self.y.shape[1]\n",
    "    mini_batch = {}\n",
    "    num = m_ex//self.mb_size\n",
    "    if (m_ex%self.mb_size != 0):\n",
    "      f = 0\n",
    "      for p in range(0,num):\n",
    "        mini_batch[\"MB_X\" + str(p)] = self.X[f:(f+self.mb_size),:]\n",
    "        mini_batch[\"MB_Y\" + str(p)] = self.y[:,f:(f+self.mb_size)]\n",
    "        f = f + self.mb_size\n",
    "      mini_batch[\"MB_X\" + str(num)] = self.X[f:m_ex,:]\n",
    "      mini_batch[\"MB_Y\" + str(num)] = self.y[:,f:m_ex]\n",
    "      return mini_batch,num\n",
    "    else:\n",
    "      f = 0\n",
    "      for p in range(0,num-1):\n",
    "        mini_batch[\"MB_X\" + str(p)] = self.X[f:(f+self.mb_size),:]\n",
    "        mini_batch[\"MB_Y\" + str(p)] = self.y[:,f:(f+self.mb_size)]\n",
    "        f = f + self.mb_size\n",
    "      mini_batch[\"MB_X\" + str(num-1)] = self.X[f:m_ex,:]\n",
    "      mini_batch[\"MB_Y\" + str(num-1)] = self.y[:,f:m_ex]\n",
    "      return mini_batch,num-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b1f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticPlotHelpers:\n",
    "  def __init__(self,costs_tr,costs_cv,accu_tr_arr,accu_cv_arr):\n",
    "    self.costs_tr,self.costs_cv = costs_tr,costs_cv\n",
    "    self.accu_tr_arr,self.accu_cv_arr = accu_tr_arr,accu_cv_arr\n",
    "\n",
    "class AnimatePlotHelpers:\n",
    "  def __init__(self,x_ax,y_ax,X_lab,Y_lab,plot_title,leg,loca,plot_col,direc,freq):\n",
    "    self.x_ax,self.y_ax,self.X_lab,self.Y_lab = x_ax,y_ax,X_lab,Y_lab\n",
    "    self.plot_title,self.leg,self.loca,self.plot_col,self.direc,self.freq = plot_title,leg,loca,plot_col,direc,freq\n",
    "\n",
    "\n",
    "class PlotCostStatic(StaticPlotHelpers):\n",
    "  def __init__(self,costs_tr,costs_cv,accu_tr_arr,accu_cv_arr):\n",
    "    StaticPlotHelpers.__init__(self,costs_tr,costs_cv,accu_tr_arr,accu_cv_arr)\n",
    "  def __call__(self):\n",
    "    itera = np.arange(1,len(self.costs_tr)+1,1)\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost Function Variation')\n",
    "    plt.plot(itera,self.costs_tr,color='c',linewidth=2)\n",
    "    if len(self.costs_cv) != 0:\n",
    "      plt.plot(itera,self.costs_cv,color='#9ef705',linewidth=2)\n",
    "    plt.legend([\"Train\",\"Cross Val\"],loc='upper right')\n",
    "\n",
    "class PlotTrCvStatic(StaticPlotHelpers):\n",
    "  def __init__(self,costs_tr,costs_cv,accu_tr_arr,accu_cv_arr):\n",
    "    StaticPlotHelpers.__init__(self,costs_tr,costs_cv,accu_tr_arr,accu_cv_arr)\n",
    "  def __call__(self):\n",
    "    itera = np.arange(1,len(self.costs_tr)+1,1)\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Train - Cross Val Accuracy Curve')\n",
    "    plt.plot(itera,self.accu_tr_arr,color='m',linewidth=2)\n",
    "    if len(self.accu_cv_arr) != 0:\n",
    "      plt.plot(itera,self.accu_cv_arr,color='r',linewidth=2)\n",
    "    plt.legend([\"Train\",\"Cross Val\"],loc='lower right')\n",
    "\n",
    "class AnimatePlot(AnimatePlotHelpers):\n",
    "  def __init__(self,x_ax,y_ax,X_lab,Y_lab,plot_title,leg,loca,plot_col,direc,freq):\n",
    "    AnimatePlotHelpers.__init__(self,x_ax,y_ax,X_lab,Y_lab,plot_title,leg,loca,plot_col,direc,freq)\n",
    "  def __call__(self):\n",
    "    y_vals = self.y_ax\n",
    "    x_vals = self.x_ax\n",
    "    l = 0\n",
    "    for k in range(1,len(x_vals)+1):\n",
    "      plt.xlabel(self.X_lab)\n",
    "      plt.ylabel(self.Y_lab)\n",
    "      plt.title(self.plot_title)\n",
    "      plt.legend([self.leg],loc=self.loca)\n",
    "      plt.plot(x_vals[0:l],y_vals[0:l],color=self.plot_col)\n",
    "      l = l+1\n",
    "      if k%self.freq==0:\n",
    "        plt.savefig(self.direc + 'plot{}.png'.format(k//self.freq))\n",
    "    return\n",
    "\n",
    "class AnimatePlotMulti(AnimatePlotHelpers):\n",
    "  def __init__(self,x_ax,y_ax,X_lab,Y_lab,plot_title,leg,loca,plot_col,direc,freq):\n",
    "    AnimatePlotHelpers.__init__(self,x_ax,y_ax,X_lab,Y_lab,plot_title,leg,loca,plot_col,direc,freq)\n",
    "  def __call__(self):\n",
    "    x_vals = {}\n",
    "    y_vals = {}\n",
    "    for m in range(0,len(self.x_ax)):\n",
    "      x_vals[\"X\" + str(m)] = self.x_ax[m]\n",
    "    for g in range(0,len(self.y_ax)):\n",
    "      y_vals[\"Y\" + str(g)] = self.y_ax[g]\n",
    "    for h in range(0,len(self.leg)):\n",
    "      plt.plot([],[],color=self.plot_col[h],label = self.leg[h])\n",
    "    plt.legend(loc=self.loca)\n",
    "    l = 0\n",
    "    for k in range(1,len(self.x_ax[0])+1):\n",
    "      plt.xlabel(self.X_lab)\n",
    "      plt.ylabel(self.Y_lab)\n",
    "      plt.title(self.plot_title)\n",
    "      for d in range(0,len(self.x_ax)):\n",
    "        if len(x_vals[\"X\" + str(d)]) != 0:\n",
    "          plt.plot(x_vals[\"X\" + str(d)][0:l],y_vals[\"Y\" + str(d)][0:l],color=self.plot_col[d])\n",
    "      l = l+1\n",
    "      if k%self.freq==0:\n",
    "        plt.savefig(self.direc + 'plot{}.png'.format(k//self.freq))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbfaea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizationHelpers:\n",
    "  def __init__(self,layers_arr,lamb,m_exam):\n",
    "    self.layers_arr,self.lamb,self.m_exam = layers_arr,lamb,m_exam\n",
    "\n",
    "\n",
    "class L1Reg(RegularizationHelpers):\n",
    "  def __init__(self,layers_arr,lamb,m_exam):\n",
    "    RegularizationHelpers.__init__(self,layers_arr,lamb,m_exam)\n",
    "  def __call__(self):\n",
    "    temp_sum = 0\n",
    "    for layers in self.layers_arr:\n",
    "      temp_sum = temp_sum + ((self.lamb/self.m_exam)*(np.sum(np.sum(layers.weights))))\n",
    "      layers.grad_reg = ((self.lamb/self.m_exam)*(layers.grad_L1))\n",
    "    return temp_sum\n",
    "\n",
    "class L2Reg(RegularizationHelpers):\n",
    "  def __init__(self,layers_arr,lamb,m_exam):\n",
    "    RegularizationHelpers.__init__(self,layers_arr,lamb,m_exam)\n",
    "  def __call__(self):\n",
    "    temp_sum = 0\n",
    "    for layers in self.layers_arr:\n",
    "      temp_sum = temp_sum + ((self.lamb/(2*self.m_exam))*(np.sum(np.sum(np.square(layers.weights)))))\n",
    "      layers.grad_reg = ((self.lamb/self.m_exam)*(layers.weights))\n",
    "    return temp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df47c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CostFunctionHelpers:\n",
    "  def __init__(self,y,A,layers_arr,lamb,reg=None):\n",
    "    self.y,self.A,self.layers_arr,self.lamb,self.reg = y,A,layers_arr,lamb,reg\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(CostFunctionHelpers):\n",
    "  def __init__(self,y,A,layers_arr,lamb,reg=None):\n",
    "    CostFunctionHelpers.__init__(self,y,A,layers_arr,lamb,reg=None)\n",
    "  def __call__(self):\n",
    "    if self.reg is not None:\n",
    "      if self.reg is \"L1\":\n",
    "        GradL1Reg(self.layers_arr)()\n",
    "        temp_sum = L1Reg(self.layers_arr,self.lamb,self.y.shape[1])()\n",
    "      if self.reg is \"L2\":\n",
    "        temp_sum = L2Reg(self.layers_arr,self.lamb,self.y.shape[1])()\n",
    "      cost = (-1/self.y.shape[1])*(np.sum(np.sum((self.y*np.log(self.A)) + ((1-self.y)*(np.log(1-self.A)))))) + temp_sum\n",
    "      grad = (-1/self.y.shape[1])*((self.y/self.A)-((1-self.y)/(1-self.A)))\n",
    "    else:\n",
    "      cost = (-1/self.y.shape[1])*(np.sum(np.sum((self.y*np.log(self.A)) + ((1-self.y)*(np.log(1-self.A))))))\n",
    "      grad = (-1/self.y.shape[1])*((self.y/self.A)-((1-self.y)/(1-self.A)))\n",
    "      for layers in self.layers_arr:\n",
    "        layers.grad_reg = 0\n",
    "    return cost,grad\n",
    "\n",
    "class CrossEntropy(CostFunctionHelpers):\n",
    "  def __init__(self,y,A,layers_arr,lamb,reg=None):\n",
    "    CostFunctionHelpers.__init__(self,y,A,layers_arr,lamb,reg=None)\n",
    "  def __call__(self):\n",
    "    if self.reg is not None:\n",
    "      if self.reg is \"L1\":\n",
    "        GradL1Reg(self.layers_arr)()\n",
    "        temp_sum = L1Reg(self.layers_arr,self.lamb,self.y.shape[1])()\n",
    "      if self.reg is \"L2\":\n",
    "        temp_sum = L2Reg(self.layers_arr,self.lamb,self.y.shape[1])()\n",
    "      cost = (-1/self.y.shape[1])*(np.sum(np.sum((self.y*np.log(self.A)))))\n",
    "      grad = (-1/self.y.shape[1])*((self.y/self.A))\n",
    "    else:\n",
    "      cost = (-1/self.y.shape[1])*(np.sum(np.sum((self.y*np.log(self.A)))))\n",
    "      grad = (-1/self.y.shape[1])*((self.y/self.A))\n",
    "      for layers in self.layers_arr:\n",
    "        layers.grad_reg = 0\n",
    "    return cost,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de228094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "  \"\"\"\n",
    "      This layer is used for implemetation of Fully Connected Neural Networks\n",
    "      Methods:\n",
    "        Parameters Initialization\n",
    "        Forward Propagation\n",
    "        Backward Propagation\n",
    "  \"\"\"\n",
    "  def __init__(self,num_inputs,num_outputs,activation_fn,dropout=1.0,weights=None,bias=None,dZ=None,dW=None,db=None,dA=None,grad_L1=None,grad_reg=None):\n",
    "    self.num_inputs = num_inputs\n",
    "    self.num_outputs = num_outputs\n",
    "    self.activation_fn = activation_fn\n",
    "    self.dropout = dropout\n",
    "    self.dZ,self.dW,self.db,self.dA = dZ,dW,db,dA\n",
    "    self.grad_L1 = grad_L1\n",
    "    self.grad_reg = grad_reg\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    self.activ_dict = {\"relu\":[Relu.forward,Relu.backward,2],\n",
    "                       \"tanh\":[Tanh.forward,Tanh.backward,1],\n",
    "                       \"sigmoid\":[Sigmoid.forward,Sigmoid.backward,1],\n",
    "                       \"softmax\":[Softmax.forward,Softmax.backward,1],\n",
    "                       \"sine\":[Sine.forward,Sine.backward,6]}\n",
    "\n",
    "  def initialize_params(self):\n",
    "    self.weights = np.random.randn(self.num_outputs,self.num_inputs)*(np.sqrt(self.activ_dict[self.activation_fn][2]/self.num_inputs))\n",
    "    self.bias = np.random.randn(self.num_outputs, 1)*0.01\n",
    "    return self.weights,self.bias,self.grad_reg,self.grad_L1\n",
    "\n",
    "  def get_params(self):\n",
    "    return self.weights, self.bias\n",
    "    \n",
    "  def forw_prop(self,A_prev,train=True):\n",
    "    if train is False:\n",
    "      self.dropout = 1\n",
    "    self.outputs = np.dot(self.weights,A_prev) + self.bias\n",
    "    self.activations_temp = self.activ_dict[self.activation_fn][0](self.outputs)\n",
    "    self.activations = self.activations_temp*((np.random.rand(self.outputs.shape[0],self.outputs.shape[1]) < self.dropout)/self.dropout)\n",
    "    return self.outputs,self.activations\n",
    "\n",
    "  def back_prop(self,dA_prev,A_prev):\n",
    "    self.dZ = dA_prev*self.activ_dict[self.activation_fn][1](self.outputs)\n",
    "    self.dW = (np.dot(self.dZ,A_prev.T)) + self.grad_reg\n",
    "    self.db = np.sum(self.dZ,axis=1,keepdims = True)\n",
    "    self.dA = np.dot(self.weights.T,self.dZ)\n",
    "    return self.dZ,self.dW,self.db,self.dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56319f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerHelpers:\n",
    "  def __init__(self,alpha,layers_arr,V_dict,S_dict,t):\n",
    "    self.alpha,self.layers_arr,self.V_dict,self.S_dict,self.t = alpha,layers_arr,V_dict,S_dict,t\n",
    "\n",
    "\n",
    "class GradientDescent(OptimizerHelpers):\n",
    "  def __init__(self,alpha,layers_arr,V_dict,S_dict,t):\n",
    "    OptimizerHelpers.__init__(self,alpha,layers_arr,V_dict,S_dict,t)\n",
    "  def __call__(self):\n",
    "    for layers in self.layers_arr:\n",
    "      layers.weights -= (self.alpha*layers.dW)\n",
    "      layers.bias -= (self.alpha*layers.db)\n",
    "\n",
    "class Momentum(OptimizerHelpers):\n",
    "  def __init__(self,alpha,layers_arr,V_dict,S_dict,t):\n",
    "    OptimizerHelpers.__init__(self,alpha,layers_arr,V_dict,S_dict,t)\n",
    "  def __call__(self):\n",
    "    beta1 = 0.9\n",
    "    for h in range(1,len(self.layers_arr)+1):\n",
    "      self.V_dict[\"Vdw\" + str(h)] = (beta1*self.V_dict[\"Vdw\" + str(h)]) + ((1-beta1)*self.layers_arr[h-1].dW)\n",
    "      self.V_dict[\"Vdb\" + str(h)] = (beta1*self.V_dict[\"Vdb\" + str(h)]) + ((1-beta1)*self.layers_arr[h-1].db)\n",
    "    for g in range(1,len(self.layers_arr)+1):\n",
    "      self.layers_arr[g-1].weights -= (self.alpha*self.V_dict[\"Vdw\" + str(g)])\n",
    "      self.layers_arr[g-1].bias -= (self.alpha*self.V_dict[\"Vdb\" + str(g)])\n",
    "\n",
    "class RMSProp(OptimizerHelpers):\n",
    "  def __init__(self,alpha,layers_arr,V_dict,S_dict,t):\n",
    "    OptimizerHelpers.__init__(self,alpha,layers_arr,V_dict,S_dict,t)\n",
    "  def __call__(self):\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for h in range(1,len(self.layers_arr)+1):\n",
    "      self.S_dict[\"Sdw\" + str(h)] = (beta2*self.S_dict[\"Sdw\" + str(h)]) + ((1-beta2)*np.square(self.layers_arr[h-1].dW))\n",
    "      self.S_dict[\"Sdb\" + str(h)] = (beta2*self.S_dict[\"Sdb\" + str(h)]) + ((1-beta2)*np.square(self.layers_arr[h-1].db))\n",
    "    for g in range(1,len(self.layers_arr)+1):\n",
    "      self.layers_arr[g-1].weights -= ((self.alpha*self.layers_arr[g-1].dW)/(np.sqrt(self.S_dict[\"Sdw\" + str(g)]) + epsilon))\n",
    "      self.layers_arr[g-1].bias -= ((self.alpha*self.layers_arr[g-1].db)/(np.sqrt(self.S_dict[\"Sdb\" + str(g)]) + epsilon))\n",
    "\n",
    "class Adam(OptimizerHelpers):\n",
    "  def __init__(self,alpha,layers_arr,V_dict,S_dict,t):\n",
    "    OptimizerHelpers.__init__(self,alpha,layers_arr,V_dict,S_dict,t)\n",
    "  def __call__(self):\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    S_dict_corr = {}\n",
    "    V_dict_corr = {}\n",
    "    for h in range(1,len(self.layers_arr)+1):\n",
    "      self.V_dict[\"Vdw\" + str(h)] = (beta1*self.V_dict[\"Vdw\" + str(h)]) + ((1-beta1)*self.layers_arr[h-1].dW)\n",
    "      self.V_dict[\"Vdb\" + str(h)] = (beta1*self.V_dict[\"Vdb\" + str(h)]) + ((1-beta1)*self.layers_arr[h-1].db)\n",
    "    for u in range(1,len(self.layers_arr)+1):\n",
    "      self.S_dict[\"Sdw\" + str(u)] = (beta2*self.S_dict[\"Sdw\" + str(u)]) + ((1-beta2)*np.square(self.layers_arr[u-1].dW))\n",
    "      self.S_dict[\"Sdb\" + str(u)] = (beta2*self.S_dict[\"Sdb\" + str(u)]) + ((1-beta2)*np.square(self.layers_arr[u-1].db))\n",
    "    for n in range(1,len(self.layers_arr)+1):\n",
    "      S_dict_corr[\"Sdw\" + str(n)] = self.S_dict[\"Sdw\" + str(n)]/(1 - np.power(beta2,self.t))\n",
    "      S_dict_corr[\"Sdb\" + str(n)] = self.S_dict[\"Sdb\" + str(n)]/(1 - np.power(beta2,self.t))\n",
    "      V_dict_corr[\"Vdw\" + str(n)] = self.V_dict[\"Vdw\" + str(n)]/(1 - np.power(beta1,self.t))\n",
    "      V_dict_corr[\"Vdb\" + str(n)] = self.V_dict[\"Vdb\" + str(n)]/(1 - np.power(beta1,self.t))\n",
    "    for g in range(1,len(self.layers_arr)+1):\n",
    "      self.layers_arr[g-1].weights -= ((self.alpha*V_dict_corr[\"Vdw\" + str(g)])/(np.sqrt(S_dict_corr[\"Sdw\" + str(g)]) + epsilon))\n",
    "      self.layers_arr[g-1].bias -= ((self.alpha*V_dict_corr[\"Vdb\" + str(g)])/(np.sqrt(S_dict_corr[\"Sdb\" + str(g)]) + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b71e9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  \"\"\"\n",
    "      Binds all the other classes together and contains methods for adding layers,\n",
    "      training the network and testing the network\n",
    "      Methods:\n",
    "        Add\n",
    "        Fit\n",
    "        Test\n",
    "  \"\"\" \n",
    "  def __init__(self,X_tr,y_tr,X_te,y_te,X_cv,y_cv):\n",
    "    self.X_tr, self.y_tr = X_tr, y_tr\n",
    "    self.X_te, self.y_te = X_te, y_te\n",
    "    self.X_cv, self.y_cv = X_cv, y_cv\n",
    "    self.layer_names = []\n",
    "    self.layer_names_arr = []\n",
    "    self.activations_cache = None\n",
    "    self.params = None\n",
    "    self.arch = {}\n",
    "    self.accu_tr_arr = None\n",
    "    self.cost_tr_arr = None\n",
    "    self.accu_cv_arr = None\n",
    "    self.cost_cv_arr = None\n",
    "    self.lr = None\n",
    "    self.epochs = None\n",
    "\n",
    "  def add(self,layer_name,num_inputs,num_outputs,act_fn,dropout=1):\n",
    "    self.layer_names_arr.append(layer_name)\n",
    "    self.arch[str(layer_name)] = [layer_name.encode('utf8'),num_inputs,num_outputs,act_fn.encode('utf8'),dropout]\n",
    "    layer_name = Dense(num_inputs,num_outputs,act_fn,dropout)\n",
    "    Dense.initialize_params(layer_name)\n",
    "    self.layer_names.append(layer_name)\n",
    "  \n",
    "  def reset(self):\n",
    "    self.layer_names = []\n",
    "    self.arch = {}\n",
    "    self.layer_names_arr = []\n",
    "    return self.layer_names,self.arch,self.layer_names_arr\n",
    "\n",
    "  def params_dict(self,print_params):\n",
    "    self.params = {}\n",
    "    hee = 1\n",
    "    for layer in self.layer_names:\n",
    "      self.params[\"W\" + str(hee)],self.params[\"b\" + str(hee)] = Dense.get_params(layer)\n",
    "      hee += 1\n",
    "    if print_params is True:\n",
    "      print(self.params)\n",
    "      return self.params\n",
    "    else:\n",
    "      return self.params\n",
    "\n",
    "  def forward_prop(self,X,train_model=True):\n",
    "    self.activations_cache = {}\n",
    "    self.activations_cache = {\"A0\":X.T}\n",
    "    temp_A = X.T\n",
    "    p = 1\n",
    "    for layer in self.layer_names:\n",
    "      _,temp_A = Dense.forw_prop(layer,temp_A,train_model)\n",
    "      self.activations_cache[\"A\" + str(p)] = temp_A\n",
    "      p += 1\n",
    "    return self.activations_cache\n",
    "\n",
    "  def backward_prop(self,y,prob_type,activations_cache,lamb,reg):\n",
    "    prob_type_dict = {\"Binary\":[BinaryCrossEntropy,PrecisionRecall,Predict,Evaluate],\n",
    "                      \"Multi\":[CrossEntropy,PrecisionRecallMulti,PredictMulti,EvaluateMulti]}\n",
    "    _,temp_dA = prob_type_dict[prob_type][0](y,activations_cache[\"A\" + str(len(self.layer_names))],self.layer_names,lamb,reg)()\n",
    "    l = 1\n",
    "    for layer in reversed(self.layer_names):\n",
    "      _,layer.dW,layer.db,temp_dA = Dense.back_prop(layer,temp_dA,self.activations_cache[\"A\" + str(len(self.layer_names)-l)])\n",
    "      l += 1\n",
    "  \n",
    "  def fit(self,X,y,alpha,num_iter,optim,prob_type,mb,reg=None,lamb=None,alpha_decay=False,print_cost=True,callback=None):\n",
    "    self.lr = alpha\n",
    "    if (num_iter > 100) and (num_iter <= 1000):\n",
    "      freq = 10\n",
    "    elif num_iter > 1000:\n",
    "      freq = 50\n",
    "    elif (num_iter <= 100):\n",
    "      freq = 1\n",
    "    params = self.params_dict(print_params=False)\n",
    "    V_dict = {}\n",
    "    S_dict = {}\n",
    "    mini_batches,num = CreateMiniBatches(X,y,mb)()\n",
    "    self.accu_tr_arr = []\n",
    "    self.cost_tr_arr = []\n",
    "    self.accu_cv_arr = []\n",
    "    self.cost_cv_arr = []\n",
    "    for k in range(1,len(self.layer_names)+1):\n",
    "      V_dict[\"Vdw\" + str(k)] = np.zeros(params[\"W\" + str(k)].shape)\n",
    "      V_dict[\"Vdb\" + str(k)] = np.zeros(params[\"b\" + str(k)].shape)\n",
    "      S_dict[\"Sdw\" + str(k)] = np.zeros(params[\"W\" + str(k)].shape)\n",
    "      S_dict[\"Sdb\" + str(k)] = np.zeros(params[\"b\" + str(k)].shape)\n",
    "    optim_dict = {\"GD\":[GradientDescent,None,None,0],\n",
    "                  \"Momentum\":[Momentum,V_dict,None,0],\n",
    "                  \"RMSprop\":[RMSProp,None,S_dict,0],\n",
    "                  \"Adam\":[Adam,V_dict,S_dict,0]}\n",
    "    prob_type_dict = {\"Binary\":[BinaryCrossEntropy,PrecisionRecall,Predict,Evaluate],\n",
    "                      \"Multi\":[CrossEntropy,PrecisionRecallMulti,PredictMulti,EvaluateMulti]}\n",
    "\n",
    "    for i in range(1,num_iter+1):\n",
    "      params_plot = self.params_dict(print_params=False)\n",
    "      if alpha_decay is True:\n",
    "        alpha = (np.power(0.95,i))*self.lr\n",
    "      for vee in tqdm(range(0,num+1),file=sys.stdout):\n",
    "        activations_dict = self.forward_prop(mini_batches[\"MB_X\" + str(vee)])\n",
    "        self.backward_prop(mini_batches[\"MB_Y\" + str(vee)],prob_type,activations_dict,lamb,reg)\n",
    "        optim_dict[optim][0](alpha,self.layer_names,optim_dict[optim][1],optim_dict[optim][2],optim_dict[optim][3]+i)()\n",
    "        time.sleep(0.005)\n",
    "      act_tr = self.forward_prop(X)\n",
    "      cost_tr,_ = prob_type_dict[prob_type][0](y,act_tr[\"A\" + str(len(self.layer_names))],self.layer_names,lamb,reg)()\n",
    "      preds = prob_type_dict[prob_type][2](act_tr[\"A\" + str(len(self.layer_names))])()\n",
    "      accu_tr = prob_type_dict[prob_type][3](y,preds)()\n",
    "      self.accu_tr_arr.append(accu_tr)\n",
    "      self.cost_tr_arr.append(cost_tr)\n",
    "      if self.X_cv is not None:\n",
    "        act_cv = self.forward_prop(self.X_cv)\n",
    "        cost_cv,_ = prob_type_dict[prob_type][0](self.y_cv,act_cv[\"A\" + str(len(self.layer_names))],self.layer_names,lamb,reg)()\n",
    "        preds_cv = prob_type_dict[prob_type][2](act_cv[\"A\" + str(len(self.layer_names))])()\n",
    "        accu_cv = prob_type_dict[prob_type][3](self.y_cv,preds_cv)()\n",
    "        self.accu_cv_arr.append(accu_cv)\n",
    "        self.cost_cv_arr.append(cost_cv)\n",
    "      if ((i%1==0) and print_cost==True):\n",
    "        if self.X_cv is None:\n",
    "          print(\"Iteration \" + str(i) + \" \" + \"train_cost: \" + str(np.round(cost_tr,6)) + \" --- \" + \"train_acc: \" + str(np.round(accu_tr,3)))\n",
    "        else:\n",
    "          print(\"Iteration \" + str(i) + \" \" + \"train_cost: \" + str(np.round(cost_tr,6)) + \" --- \" + \"train_acc: \" + str(np.round(accu_tr,3)) + \" --- \" + \"val_cost: \" + str(np.round(cost_cv,6)) + \" --- \" + \"val_accu: \" + str(np.round(accu_cv,3)))\n",
    "      if (i % 1 == 0):\n",
    "        if (callback is not None):\n",
    "          callback(i, params_plot)\n",
    "\n",
    "  def save_model(self,fname):\n",
    "    params = self.params_dict(print_params=False)\n",
    "    archi = self.arch\n",
    "    model_dict = {\"Parameters\":params,\"Architecture\":archi}\n",
    "    hdfdict.dump(model_dict,fname)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "  def load_model(self,fname):\n",
    "    print(\"Model loading....\")\n",
    "    model_dict = dict(hdfdict.load(fname))\n",
    "    params_dict = model_dict[\"Parameters\"]\n",
    "    arch_dict = model_dict[\"Architecture\"]\n",
    "    self.reset()\n",
    "    for key in arch_dict:\n",
    "      self.add(arch_dict[key][0].decode('utf8'),int(arch_dict[key][1]),int(arch_dict[key][2]),arch_dict[key][3].decode('utf8'),int(arch_dict[key][4]))\n",
    "    dee = 1\n",
    "    for layer in self.layer_names:\n",
    "      layer.weights = params_dict[\"W\" + str(dee)]\n",
    "      layer.bias = params_dict[\"b\" + str(dee)]\n",
    "      dee += 1\n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "  def plot(self,type_func,animate=False,direc=None):\n",
    "    itera = np.arange(1,len(self.cost_tr_arr)+1)\n",
    "    if (len(itera) > 100) and (len(itera) <= 1000):\n",
    "      freq = 10\n",
    "    elif len(itera) > 1000:\n",
    "      freq = 50\n",
    "    elif (len(itera) <= 100):\n",
    "      freq = 1\n",
    "    plot_dict = {\"Cost\":[PlotCostStatic,AnimatePlotMulti,self.cost_tr_arr,self.cost_cv_arr,'Costs','Train-Cross Val Costs Curve','upper right',['c','#9ef705'],AnimatePlot,\"Costs\",\"Cost Function Curve\"],\n",
    "                 \"Accuracy\":[PlotTrCvStatic,AnimatePlotMulti,self.accu_tr_arr,self.accu_cv_arr,'Accuracy','Train-Cross Val Accuracy Curve','lower right',['m','r'],AnimatePlot,\"Accuracy\",\"Accuracy Curve\"]}\n",
    "    if animate is False:\n",
    "      plot_dict[type_func][0](self.cost_tr_arr,self.cost_cv_arr,self.accu_tr_arr,self.accu_cv_arr)()\n",
    "    else:\n",
    "      if len(self.cost_cv_arr) != 0:\n",
    "        plot_dict[type_func][1]([itera,itera],[plot_dict[type_func][2],plot_dict[type_func][3]],'Number of Iterations',plot_dict[type_func][4],plot_dict[type_func][5],['Train', 'Cross Val'],plot_dict[type_func][6],plot_dict[type_func][7],direc,freq)()\n",
    "      else:\n",
    "        plot_dict[type_func][8](itera,plot_dict[type_func][2],\"Number of Iterations\",plot_dict[type_func][9],plot_dict[type_func][10],\"Train\",plot_dict[type_func][6],plot_dict[type_func][7][0],direc,freq)()\n",
    "      print(\"Go to your directory to find the images! Feed them to a GIF creator to animate them!\")\n",
    "\n",
    "\n",
    "  def summary(self):\n",
    "    tab = PrettyTable()\n",
    "    tab.field_names = [\"Layer Number\",\"Layer Name\",\"Inputs\",\"Outputs\",\"Activation\",\"Dropout\",\"Number of Parameters\"]\n",
    "    yee = 1\n",
    "    total_params = 0\n",
    "    for layer in self.layer_names:\n",
    "      total_params += (layer.weights.shape[0]*layer.weights.shape[1])+len(layer.bias)\n",
    "      tab.add_row([str(yee),self.layer_names_arr[yee-1],layer.weights.shape[1],layer.weights.shape[0],layer.activation_fn,layer.dropout,str((layer.weights.shape[0]*layer.weights.shape[1])+len(layer.bias))])\n",
    "      yee += 1\n",
    "    print(tab)\n",
    "    print(\"Total number of trainable Parameters: \" + str(total_params))\n",
    "\n",
    "  def test(self,X,y,prob_type,training=False,print_values=True):\n",
    "    prob_type_dict = {\"Binary\":[BinaryCrossEntropy,PrecisionRecall,Predict,Evaluate],\n",
    "                      \"Multi\":[CrossEntropy,PrecisionRecallMulti,PredictMulti,EvaluateMulti]}\n",
    "    act_te = self.forward_prop(X,training)\n",
    "    predictions_te = prob_type_dict[prob_type][2](act_te[\"A\" + str(len(self.layer_names))])()\n",
    "    accu_te = prob_type_dict[prob_type][3](y,predictions_te)()\n",
    "    prec_te,rec_te,f1_te = prob_type_dict[prob_type][1](predictions_te,y)()\n",
    "    tab = PrettyTable()\n",
    "    if prob_type == \"Multi\":\n",
    "      tab.field_names = [\"Class\",\"Precision\",\"Recall\",\"F1\"]\n",
    "      for hee in range(0,y.shape[0]):\n",
    "        tab.add_row([hee,prec_te[\"class\" + str(hee)],rec_te[\"class\" + str(hee)],f1_te[\"class\" + str(hee)]])\n",
    "      if print_values is True:\n",
    "        print(tab)\n",
    "        print(\"Test Accuracy: \" + str(accu_te))\n",
    "    if prob_type == \"Binary\":\n",
    "      tab.field_names = [\"Precision\",\"Recall\",\"F1\"]\n",
    "      tab.add_row([str(prec_te),str(rec_te),str(f1_te)])\n",
    "      if print_values is True:\n",
    "        print(tab)\n",
    "        print(\"Test Accuracy: \" + str(accu_te))\n",
    "    if print_values is not True:\n",
    "      return accu_te,prec_te,rec_te,f1_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a84b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(X_tr,y_tr,X_te,y_te,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02048693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset()\n",
    "model.add(\"dense1\",X_tr.shape[1],500,\"relu\")\n",
    "model.add(\"dense2\",500,250,\"relu\")\n",
    "model.add(\"dense3\",250,150,\"relu\")\n",
    "model.add(\"dense4\",150,100,\"sine\")\n",
    "model.add(\"dense5\",100,60,\"sine\")\n",
    "model.add(\"dense6\",60,30,\"relu\")\n",
    "model.add(\"dense7\",30,15,\"tanh\")\n",
    "model.add(\"dense8\",15,10,\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.81it/s]\n",
      "Iteration 1 train_cost: 1.344266 --- train_acc: 75.518\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.95it/s]\n",
      "Iteration 2 train_cost: 1.0086 --- train_acc: 89.585\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.86it/s]\n",
      "Iteration 3 train_cost: 0.853178 --- train_acc: 92.395\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.76it/s]\n",
      "Iteration 4 train_cost: 0.761011 --- train_acc: 94.148\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.84it/s]\n",
      "Iteration 5 train_cost: 0.700927 --- train_acc: 95.055\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.86it/s]\n",
      "Iteration 6 train_cost: 0.659343 --- train_acc: 95.673\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.91it/s]\n",
      "Iteration 7 train_cost: 0.625299 --- train_acc: 96.165\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.89it/s]\n",
      "Iteration 8 train_cost: 0.597388 --- train_acc: 96.523\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.90it/s]\n",
      "Iteration 9 train_cost: 0.572919 --- train_acc: 96.837\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.81it/s]\n",
      "Iteration 10 train_cost: 0.55098 --- train_acc: 97.077\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.85it/s]\n",
      "Iteration 11 train_cost: 0.530966 --- train_acc: 97.297\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.82it/s]\n",
      "Iteration 12 train_cost: 0.512851 --- train_acc: 97.475\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.80it/s]\n",
      "Iteration 13 train_cost: 0.496378 --- train_acc: 97.653\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:08<00:00,  3.75it/s]\n",
      "Iteration 14 train_cost: 0.481126 --- train_acc: 97.787\n",
      " 10%|████████▎                                                                          | 3/30 [00:00<00:07,  3.40it/s]"
     ]
    }
   ],
   "source": [
    "model.fit(X_tr,y_tr,0.0005,15,\"Adam\",\"Multi\",mb=2048,alpha_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot(\"Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3022cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\Users\\dungm\\Desktop\\neural_network_sci01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a17f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('mnist_01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd33cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(X_tr,y_tr,X_te,y_te,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_model('mnist_01.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.test(X_te,y_te,\"Multi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
